---
title: Communication Strategy
description: Ways to help out with pausing AGI development.
---

- **Defer to experts**. We are warning people about a scenario that is so extreme and scary, that a gut-level response is to dismiss it as crazy talk. Show the [expert polls and surveys](/polls-and-surveys). The [top three](https://twitter.com/PauseAI/status/1734641804245455017) most cited AI scientists are all warning about x-risk. Deferring to them is a good way to make our case.
- **Use simple language**. You can show you understand the technology and you've done your homework, but excessive jargon can make people lose interest. We want to reach as many people as possible. This means we need to use simple language and avoid jargon.
- **Show our emotions**. Seeing emotions gives others the permission to feel emotions. We are worried, we are angry, we are eager to act. Showing how you feel can be scary, but in our case we need to. Our message can only be received if it matches with how we send it.
- **Emphasize uncertainty**. Don't say AI _will_ take over, or that we _will_ reach AGI in x years. Nobody can predict the future. There is a significant _chance_ that AI will go wrong soon, and that should be enough to act on. Don't let uncertainty be the reason to not act. Refer to the _Precautionary Principle_, and make the point that we should err on the side of caution.
- **Make individuals feel responsible**. Nobody wants to feel like they have a strong responsibility to make things go well. Our brains steer us away from this, because we all have a deep desire to believe that someone is in charge, protecting us. But there are no adults in the room right now. You need to be the one to do this. Choose to take responsibility.
- **Inspire hope**. When hearing about the dangers of AI and the current race to the bottom, many of us will feel dread, and that makes us not act. Fatalism is comfortable, because a lack of hope means that we don't have to work towards a good outcome. This is why we need to emphasize that our case is not lost. AGI is [not inevitable](/feasibility), technology has been successfully banned internationally before, and our proposal has broad public support.
