---
title: PauseAI forslag
description: Innfør en midlertidig pause i treningen av AI systemer kraftigere enn GPT-4, forby trening på opphavsrettighetsbeskyttet materiale, hold skapere av AI systemer juridisk ansvarlig for skade.
---

**Innfør en pause på trening av AI systemer som er kraftigere enn GPT-4 ** intil vi vet hvordan dette kan gjøres trygt og under demokratisk styring.

Hvert enkelt land kan innføre dette _umiddelbart_ og på eget initiativ.
Spesielt US (eller delstaten California spesifikt), bør innføre en pause, siden dette er hvor tilnærmet all ledende AI selskaper opererer fra.
Mange vitenskapsfolk og ledere innen AI industrien [er enige i at en pause er nødvendig](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), og den amerikanske befolkningen stiller seg også sterk støttene til en slik pause ([64%](https://www.campaignforaisafety.org/usa-ai-x-risk-perception-tracker/) - [69%](https://today.yougov.com/topics/technology/survey-results/daily/2023/04/03/ad825/2)).

Men, vi kan ikke forvente at land og selskaper risikerer sine konkurransefortrinn om ikke andre land og selskaper gjør det samme.  

Derfor trengs en **global pause**!

## Innføring av en global Pause
En internasjonal avtale blir typisk etablert gjennom et toppmøte, hvor ledere fra ulike land møtes for å diskutere temaet og komme fremt il en beslutning. Storbritania tok initativ og var værtskap for det første "AI Safety Summit" (AI sikkerhets toppmøte) høsten 2023. Her var desverre ikke Norge representert, selv om vårt lille land ofte deltar aktivt i toppmøter omkring andre globale temaer. Vi håper dette var en glipp og at Norge velger å delta aktivt innen dette fremover. 

To nye toppmøter har blitt annonsert. 
[More about the summits](https://pauseai.info//summit)

Det primære målet med toppmøtet bør være en **avtale**.
En slik avtale burde spesifisere politiske tiltak egnet til å beskytte oss mot [AI risiko](/risks).
Avtalen bør undertegnes av alle medlemsland i FN.


- **Etabler et intarnasjonalt AI sikkerhets byrå**, tilsvarnde det Internasjonale Atomenergi Byrået [IAEA](https://www.iaea.org/). Dette byrået vil ha ansvar for:
  - Gi tillatelse for til _igangsettelse av drift_ av avanserte AI systemer. Dette inkluderer "red-teaming"(testing av sårbarhet for misbruk) og evaluering av system-kapabiliteter.
  - Gi tillatelse til _igangsettelse av trening_ av avanserte AI-maskinlærings modeller. 
  - Periodiske møter for å diskutere fremgann innen forskning på AI-sikkerhet.
- **Kun tillate trening av av generelle AI systemer kraftigere enn GPT-4 om sikkerheten kan garanteres**
  - Med "kraftigere enn GPT-4" menes alle AI modeller som enten er 1) Større enn 10^12 parametre 2) Det har blitt brukt mer enn 10^25 flyttallsoperasjoner (FLOP) under treningen, eller 3) om kapabilitetene forventes å kunne overgå GPT-4
  - Merk at dette ikke retter seg mot _smale_ (spesialiserte) AI systmer, slik f.eks. et som brukes for å diagnostisere kreft utifra medisinske bilder. 
  - Det kreves [tilsyn under trening steg](https://www.alignmentforum.org/posts/Zfk6faYvcf5Ht7xDx/compute-thresholds-proposed-rules-to-mitigate-risk-of-a-lab).
  - Sikkerhet kan kun sies å være "garantert" i snever forstand om det er en sterk vitenskapelig konsensus og [tilstrekkelig bevis](https://arxiv.org/abs/2309.01933) for at _AI alignment_ problemet har blitt løst. Dette vil si at det først må etableres tverrfaglig konsensus om at man har funnet en eller flere fullgode metoder for å sikre at AI systemer med tilsvarende eller bedre kapabiliteter som mennesker innen de fleste områder kun vil gjøre de tingene vi spesifiserer dermed i det minste kunne være kontrollerbart av mennesker.
  - Foreløpig er det ukjent om det i det hele tatt vil være mulig å løse "AI Alignment". Om det ikke finnes en løsning på dette bør aldri slike systemer trenes opp.
  - Selv om man lykkes å bygge kontrollerbar AI, kun bygg og driftsett slik teknologo under **streng demokratisk kontroll** verden over. Et system for superintelligens er for kraftig til at vi kan tillate at et slikt system er kontrollert og styrt av enkelt-organisasjoner eller land.
- [**Spor salg av GPUer**](https://arxiv.org/abs/2303.11341) og annen maskinvare som kan brukes for denne type AI trening. 
- **Bare tillat driftsetting av et slikt AI system etter at ingen [farlige kapabiliteter](https://pauseai.info/dangerous-capabilities) er funnet**.
  - Vi vil trenge standarder og uavhengig "red-teaming" for å avgjøre om en modell har farlige kapabiliteter.
  - Listen med farlige kapabiliteter kan endres over tid mens AI teknologien utvikles.
  - Merk at det å kun basere seg på å evaluere kapabiliteter [ikke er tilstrekkelig](https://pauseai.info/4-levels-of-ai-regulation).

Innføring av en pause kan gi utilsiktede negative følger om det ikke gjøres skikkelig.
Les mer om  [hvordan risiko for slike utilsiktede følger kan motvirkes.](https://pauseai.info/mitigating-pause-failures) 

## Andre virkemidler som bidrar til å dempe kappløpet
- **Forby trening av AI systemer på opphavsrettighets-beskyttet materiale**. Dette tar vare på skaperne av den type materiale, kan motvikre økende forskjeller og reduserer tempoet i utvikling av superintelligens. 
- **Hold skaperne av avanserte AI systemer økonomisk og juridisk ansvarlig** for kriminelle handlinger som er utført med og muligjort med disse systemene. Dette gir et kraftig insentiv for å sørge for at AI-systemer sikres bedre enn de gjøres i dag.

## Langsiktig AI politikk
I dag koster det flere titalls millioner kroner å trene et system på nivå med GPT-3. Dette gjør det veldig vanskelig å trene slike systemer, og gjør det lettere å håndheve kontroll på trening gjennom sporing av GPU anskaffelser. Men på sikt forventes det at kostnadene ved slik trening vil falle eksponensielt pga forbedringer i maskinvare og i maskinlæringsalgoritmer. 

Vi kan potensielt komme til et punkt hvor det blir mulig å trene generelt superintelligente AI systmer for noen få titusener av kroner, kanskje på maskinvare som vanlige forbrukere har tilgang på. 

Derfor bør vi vurdere følgende virkemidler:

- **Begrens publikasjonen av forbedre algoritmer for trening og inferens (bruksfase)**. Noen ganger publiseres algoritmer som gjør trening av maskinlærings systemer lang mer effektivt. Transformer-arkitekturen alene har vært sentral i nesten alle nylige fremsrkitt innen AI. Slike hopp i kapabilitet kan skje når som helst, og vi burde vurdere å begrense publikasjonen av slike algoritmer for å minimere risiko for at kapabiliteter kommer ut av kontroll. Samtidig skjer det fra tid til annen oppdagelser rund hvordan man kan redusere ressursbruken når et maskinlærings system settes i drift, noe som kan gi store uoversiktlige konsekvenser i hvordan disse systemene kan brukes. 
- **Begrens utvikling av maskinvare**. Om det blir mulig å trening opp maskinlæringssystmer med superintelligens på maskinvare som er tilgjengelig for vanlige forbrukere står vi i en meget farlig situasjon. Dette kan sammenlignes med en situasjon hvor folk flest når som helst har mulighet til enkelt å utvikle og utløse masseødeleggelsesvåpen. 

## Bidra til at vi lykkes med gjennomføring av slike forslag
[Meld deg inn](https://pauseai.info/join) i bevegelsen for å samarbeide med oss eller [ta initativ](https://pauseai.info/action) på egenhånd!
